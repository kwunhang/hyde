{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "build index\n",
    "Add the encider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import FaissSearcher, LuceneSearcher\n",
    "from pyserini.search.faiss import AutoQueryEncoder\n",
    "from pyserini.search import get_topics, get_qrels\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BGE reranker\n",
    "def load_reranker():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-large\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-large\")\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking function\n",
    "def rerank_results(model, tokenizer, query, hits, batch_size=8):\n",
    "    model.eval()\n",
    "    documents = [hit.raw for hit in hits]\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i + batch_size]\n",
    "        queries = [query] * len(batch_docs)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            queries,\n",
    "            batch_docs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_scores = outputs.logits.squeeze(-1)\n",
    "            scores.extend(batch_scores.cpu().numpy())\n",
    "    \n",
    "    return list(zip(hits, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import ast\n",
    "from openai import AsyncOpenAI, OpenAI, APIConnectionError, RateLimitError\n",
    "\n",
    "\n",
    "LLM_BASE_URL = \"http://localhost:8000/v1/\"\n",
    "LLM_API_KEY = \"sk-22\"\n",
    "MODEL = \"DSF-CUG-LLM\"\n",
    "\n",
    "openai.api_key = \" \" ## Insert OpenAI's API key\n",
    "openai_async_client = OpenAI(\n",
    "        api_key=LLM_API_KEY, base_url=LLM_BASE_URL\n",
    "    )\n",
    "\n",
    "def call_codex_read_api(prompt: str, n =1):\n",
    "    def parse_api_result(result):\n",
    "        to_return = []\n",
    "        for idx, g in enumerate(result['choices']):\n",
    "            text = g['text']\n",
    "            logprob = sum(g['logprobs']['token_logprobs'])\n",
    "            to_return.append((text, logprob))\n",
    "        res = [r[0] for r in sorted(to_return, key=lambda tup: tup[1], reverse=True)]\n",
    "        return res\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        generated_output = openai_async_client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        result.append(generated_output)\n",
    "    \n",
    "    return parse_api_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYDE generation function (similar to the reference code)\n",
    "def generate_hyde_contexts(query):\n",
    "    prompt = f\"\"\"Please write a passage to answer the question\n",
    "Question: {query}\n",
    "Passage:\"\"\"\n",
    "    get_result = False\n",
    "    while not get_result:\n",
    "        try:\n",
    "            contexts = [c.strip() for c in call_codex_read_api(prompt, n=8)] + [query]\n",
    "            get_result = True\n",
    "        except:\n",
    "            sleep(1)\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insturction for rerank generation function (similar to the reference code)\n",
    "def generate_query_instruct(query)->str:\n",
    "    prompt = f\"\"\"Please write an instruction to specifies the nature of the task. It should be a short sentence.\n",
    "task: {query}\n",
    "Instruction:\"\"\"\n",
    "    get_result = False\n",
    "    while not get_result:\n",
    "        try:\n",
    "            contexts = [c.strip() for c in call_codex_read_api(prompt, n=1)]\n",
    "            get_result = True\n",
    "        except:\n",
    "            sleep(1)\n",
    "    return contexts[0] + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_hyde_embedding(query_encoder, contexts):\n",
    "    \"\"\"\n",
    "    Encode all contexts and average their embeddings\n",
    "    \"\"\"\n",
    "    all_emb_c = []\n",
    "    for c in contexts:\n",
    "        c_emb = query_encoder.encode(c)\n",
    "        all_emb_c.append(np.array(c_emb))\n",
    "    all_emb_c = np.array(all_emb_c)\n",
    "    avg_emb_c = np.mean(all_emb_c, axis=0)\n",
    "    return avg_emb_c.reshape((1, len(avg_emb_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics('dl19-passage')\n",
    "qrels = get_qrels('dl19-passage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_searcher = FaissSearcher.from_prebuilt_index(\n",
    "    'msmarco-v1-passage.bge-base-en-v1.5',\n",
    "    'BAAI/bge-base-en-v1.5'\n",
    ")\n",
    "reranker_model, reranker_tokenizer = load_reranker()\n",
    "query_encoder = AutoQueryEncoder('BAAI/bge-base-en-v1.5', device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result A: BGE base retrieval\n",
    "with open('dl19-bge-base-top1000-trec', 'w') as f_A, open('dl19-bge-base-reranked-trec', 'w') as f_B:\n",
    "    for qid in tqdm(topics):\n",
    "        if qid in qrels:\n",
    "            query = topics[qid]['title']\n",
    "            hits = bge_searcher.search(query, k=1000)\n",
    "            # Result A: BGE base retrieval\n",
    "            for rank, hit in enumerate(hits, 1):\n",
    "                f_A.write(f'{qid} Q0 {hit.docid} {rank} {hit.score} rank\\n')\n",
    "            \n",
    "            # Result B: BGE base + reranker\n",
    "            rerank_scores = rerank_results(reranker_model, reranker_tokenizer, query, hits)\n",
    "            \n",
    "            # Sort by reranker scores\n",
    "            ranked_results = sorted(rerank_scores, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (hit, score) in enumerate(ranked_results, 1):\n",
    "                f_B.write(f'{qid} Q0 {hit.docid} {rank} {score} rank\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result X: BGE base + query with instruct retrieval\n",
    "q_i_generations = {}\n",
    "with open('dl19-bge-instrct-top1000-trec', 'w') as f_X, open('dl19-bge-instrct-reranked-base-trec', 'w') as f_Y, open('dl19-bge-instrct-reranked-instruct-trec', 'w') as f_Z:\n",
    "    for qid in tqdm(topics):\n",
    "        if qid in qrels:\n",
    "            query = topics[qid]['title']\n",
    "            print(query)\n",
    "            query_instruct = generate_query_instruct(query)\n",
    "            print(\"query_instruct:\\n\", query_instruct)\n",
    "            q_i_generations[qid] = query_instruct  # Save generations for analysis\n",
    "\n",
    "\n",
    "            hits = bge_searcher.search(query_instruct, k=1000)\n",
    "            # Result X: BGE base + query with instruct retrieval\n",
    "            for rank, hit in enumerate(hits, 1):\n",
    "                f_X.write(f'{qid} Q0 {hit.docid} {rank} {hit.score} rank\\n')\n",
    "            \n",
    "            # Result Y: BGE base + query with instruct retrieval + reranker base\n",
    "            rerank_scores = rerank_results(reranker_model, reranker_tokenizer, query, hits)\n",
    "            \n",
    "            # Sort by reranker scores\n",
    "            ranked_results = sorted(rerank_scores, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (hit, score) in enumerate(ranked_results, 1):\n",
    "                f_Y.write(f'{qid} Q0 {hit.docid} {rank} {score} rank\\n')\n",
    "            \n",
    "\n",
    "            # Result Z: BGE base + query with instruct retrieval + reranker instruct\n",
    "            rerank_scores = rerank_results(reranker_model, reranker_tokenizer, query_instruct, hits)\n",
    "            \n",
    "            # Sort by reranker scores\n",
    "            ranked_results = sorted(rerank_scores, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (hit, score) in enumerate(ranked_results, 1):\n",
    "                f_Z.write(f'{qid} Q0 {hit.docid} {rank} {score} rank\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HYDE generations for analysis\n",
    "with open('q_i_generations.json', 'w') as f:\n",
    "    json.dump(q_i_generations, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result B: BGE base + reranker\n",
    "# with open('dl19-bge-base-reranked-trec', 'w') as f:\n",
    "#     for qid in tqdm(topics):\n",
    "#         if qid in qrels:\n",
    "#             query = topics[qid]['title']\n",
    "#             hits = bge_searcher.search(query, k=1000)\n",
    "#             documents = [hit.raw for hit in hits]\n",
    "#             rerank_scores = rerank_results(reranker_model, reranker_tokenizer, query, documents)\n",
    "            \n",
    "#             # Sort by reranker scores\n",
    "#             ranked_results = sorted(zip(hits, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "#             for rank, (hit, score) in enumerate(ranked_results, 1):\n",
    "#                 f.write(f'{qid} Q0 {hit.docid} {rank} {score} rank\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store HYDE generations for analysis\n",
    "hyde_generations = {}\n",
    "\n",
    "# Result D: HYDE + BGE base\n",
    "with open('dl19-hyde-bge-base-trec', 'w') as f_D, open('dl19-hyde-bge-base-reranked-trec', 'w') as f_E:\n",
    "    for qid in tqdm(topics):\n",
    "        if qid in qrels:\n",
    "            query = topics[qid]['title']\n",
    "            print(query)\n",
    "            contexts = generate_hyde_contexts(query)\n",
    "            \n",
    "            # Average embeddings of contexts\n",
    "            contexts = generate_hyde_contexts(query)\n",
    "            contexts.append(query)  # Add original query as an additional context\n",
    "            hyde_generations[qid] = contexts  # Save generations for analysis\n",
    "\n",
    "            # Get averaged embedding for HYDE\n",
    "            avg_emb = get_averaged_hyde_embedding(query_encoder, contexts)\n",
    "            \n",
    "            # Search with averaged embedding\n",
    "            hyde_hits = bge_searcher.search(avg_emb, k=1000)\n",
    "            \n",
    "            for rank, hit in enumerate(hyde_hits, 1):\n",
    "                f_D.write(f'{qid} Q0 {hit.docid} {rank} {hit.score} rank\\n')\n",
    "\n",
    "            # Result E: HYDE + BGE base + reranker\n",
    "            reranked_hyde = rerank_results(reranker_model, reranker_tokenizer, query, hyde_hits)\n",
    "            \n",
    "            # Sort by reranker scores\n",
    "            ranked_hyde_results = sorted(reranked_hyde, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (hit, score) in enumerate(ranked_hyde_results, 1):\n",
    "                f_E.write(f'{qid} Q0 {hit.docid} {rank} {score} rank\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HYDE generations for analysis\n",
    "with open('hyde_generations.json', 'w') as f:\n",
    "    json.dump(hyde_generations, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result D: HYDE + BGE base + reranker\n",
    "# with open('dl19-hyde-bge-base-reranked-trec', 'w') as f:\n",
    "#     for qid in tqdm(topics):\n",
    "#         if qid in qrels:\n",
    "#             query = topics[qid]['title']\n",
    "#             contexts = generate_hyde_contexts(query)\n",
    "            \n",
    "#             # Get documents from HYDE results\n",
    "#             all_hits = []\n",
    "#             for context in contexts:\n",
    "#                 hits = bge_searcher.search(context, k=1000)\n",
    "#                 all_hits.extend(hits)\n",
    "            \n",
    "#             # Remove duplicates and get raw documents\n",
    "#             unique_docs = {}\n",
    "#             for hit in all_hits:\n",
    "#                 if hit.docid not in unique_docs:\n",
    "#                     unique_docs[hit.docid] = hit.raw\n",
    "            \n",
    "#             # Rerank\n",
    "#             documents = list(unique_docs.values())\n",
    "#             rerank_scores = rerank_results(reranker_model, reranker_tokenizer, query, documents)\n",
    "            \n",
    "#             # Sort and write results\n",
    "#             doc_ids = list(unique_docs.keys())\n",
    "#             ranked_results = sorted(zip(doc_ids, rerank_scores), key=lambda x: x[1], reverse=True)[:1000]\n",
    "#             for rank, (docid, score) in enumerate(ranked_results, 1):\n",
    "#                 f.write(f'{qid} Q0 {docid} {rank} {score} rank\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all results\n",
    "for result_file in ['dl19-bge-base-top1000-trec', \n",
    "                    'dl19-bge-base-reranked-trec',\n",
    "                    'dl19-hyde-bge-base-trec',\n",
    "                    'dl19-hyde-bge-base-reranked-trec',\n",
    "                    'dl19-bge-instrct-top1000-trec',\n",
    "                    'dl19-bge-instrct-reranked-base-trec',\n",
    "                    'dl19-bge-instrct-reranked-instruct-trec']:\n",
    "    print(f\"\\nEvaluating {result_file}\")\n",
    "    os.system(f\"python -m pyserini.eval.trec_eval -c -l 2 -m map dl19-passage {result_file}\")\n",
    "    os.system(f\"python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 dl19-passage {result_file}\")\n",
    "    os.system(f\"python -m pyserini.eval.trec_eval -c -l 2 -m recall.1000 dl19-passage {result_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
